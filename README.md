# TalentAI n8n Contact Ingestion

This repository contains an n8n-based ingestion pipeline that receives contacts from a CRM, normalizes and validates the data, persists it to PostgreSQL, and records detailed observability data. A dedicated error workflow logs failures and can notify operators.

## Overview

The main workflow exposes a production webhook endpoint to receive contacts from a mock CRM API. It transforms input, identifies new contacts, inserts them into PostgreSQL, logs processing details, and returns a response to the caller. A separate error workflow starts with an Error Trigger node to log unhandled failures across workflows.

### Architecture
```
┌───────────┐    ┌─────────────┐    ┌─────────────┐
│ Mock CRM  │──▶ │    n8n      │──▶ │ PostgreSQL  │
│ (port 3000)│   │  workflow   │    │  (port 5432)│
└───────────┘    └─────────────┘    └─────────────┘
                         │
                         └─────────────▶ Grafana (port 3001)
```

Services are defined in `docker-compose.yml`:
- `postgres:15` with schema initialization from `init-db/`
- `mock-crm` (Python API in `mock-crm/`) on port 3000
- `n8n` on port 5678 (workflows mounted from `n8n-workflows/`)
- `grafana` on port 3001

## Setup

Prerequisites: Docker and Docker Compose.

Environment setup
```bash
cp .env.example .env
```

Start the stack
```bash
docker compose up -d
```

## Workflows

### Main workflow (`n8n-workflows/workflow.json`)
Key nodes:
- `CRM Webhook Trigger`: Production webhook path `contacts-webhook`.
- `Start Run`: Inserts a row in `ingestion_runs` to track the execution.
- `Fetch CRM Existing Contacts` and `Fetch CRM New Contacts`: Talks to `mock-crm`.
- `Extract & Transform Contacts` and `Normalize Contact Data`: Normalizes names, email, and phone into E.164.
- `Check Contact Exists`: Gets already-known `crm_id` values.
- `New Contacts` + `If`: Filters only new records.
- `Insert New Contact`: Inserts into `contacts` with ON CONFLICT DO NOTHING.
- `Log Inserted Contacts` and `Compute/Log Duplicates`: Adds per-contact audit rows in `contact_processing_log`.
- `Merge` → `Finalize Run`: Completes the run and updates run-level metrics. Finally, `Webhook Response` returns to the caller.

### Error workflow (`n8n-workflows/Global Error Notifier.json`)
- Starts with `Error Trigger` (no activation required; runs for automatic/production executions).
- `Log Error to DB`: Inserts error details into `processing_errors` without setting the `id` (auto-generated by BIGSERIAL).
- `Send a message`: Email notification.

### Database schema

Schema is created by `init-db/01-create-schema.sql` and `init-db/02-observability.sql`.

Primary tables:
- `contacts`: core contact store; `crm_id` is unique; computed `full_name`; indices on email, phone, and metadata.
- `contact_processing_log`: per-contact audit trail with `operation`, `status`, `message`, `workflow_execution`, and optional `run_id` and `event_type`.
- `processing_errors`: stores unhandled errors captured by the error workflow. `id` is BIGSERIAL; do not set it from n8n.
- `ingestion_runs`: one row per workflow execution with counters for received, inserted, duplicates, emails, and status.

Sequences and indices are created for performance and reporting. See the SQL files for full definitions.

### Monitoring (Grafana)

Grafana is provisioned with dashboards from `grafana/dashboards/`. Access at http://localhost:3001. The dashboards visualize ingestion runs, throughput, and error counts using the data stored in PostgreSQL.

### Design logic

- Ingestion is webhook-driven to minimize polling latency and simplify external integrations. The main workflow normalizes input (name, email, phone), deduplicates by `crm_id`, inserts into `contacts`, and writes audit rows to `contact_processing_log`. Each run is tracked in `ingestion_runs` with counters and timestamps for observability. A dedicated error workflow, starting with `Error Trigger`, logs unhandled failures to `processing_errors` and can notify by email.
- Data quality is enforced early: phone numbers are converted to E.164, emails normalized and filtered with a basic regex, and names uppercased to generate `full_name` consistently.
- The workflow responds via `Webhook Response` after run completion; this ensures the caller gets a response only when downstream persistence has concluded.

### Best practices applied
- Security: n8n basic auth, least-privilege DB user, and environment-driven secrets in `docker-compose.yml`.
- Data integrity: parameterized SQL or explicit column mapping, `BIGSERIAL` primary keys, and unique `crm_id` with relevant indexes (email, phone, tags, raw payload GIN).
- Idempotency: deduplication through existence checks plus `ON CONFLICT DO NOTHING` on `crm_id`.
- Observability: per-contact logs, per-execution run records, simple metrics usable by Grafana, and structured error logging.
- Maintainability: clear node names, separation of error handling into a dedicated workflow, and a normalized schema with computed `full_name` for consistent reads.

### Limitations and targeted improvements
- Single-instance n8n: the current setup runs inline on one n8n container. Improve resilience with queue mode and workers.
- Limited retries/backoff: add node-level retries with exponential backoff and circuit-breaker semantics for flaky external calls.
- Batch efficiency: insert batching is limited; adopt bulk insert patterns for higher throughput and fewer round trips.
- Rate limiting: introduce per-provider rate controls and centralized throttling for external APIs.

### Scaling plan (target: 100,000 records/day)
- Execution model: enable n8n queue mode with Redis and deploy 3–5 workers; keep the editor as a separate instance.
- Database: partition large tables by time, add read replicas, and front with PgBouncer. Move heavy analytics to replicas.
- Throughput: use bulk inserts for contacts and logs, reduce synchronous email sends or route them to an async notification workflow.
- Reliability: retries with jitter, dead-letter paths for repeatedly failing items, and idempotency keys for reprocessing.
- Monitoring: expand Grafana dashboards to include queue lag, worker saturation, and DB query latency, plus alerts for error rates and backlog growth.